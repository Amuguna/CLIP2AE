{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modules.until_module import PreTrainedModel, AllGather, CrossEn\n",
    "from modules.module_cross import CrossModel, CrossConfig, Transformer as TransformerClip\n",
    "\n",
    "from modules.module_clip import CLIP, convert_weights\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataloaders.rawvideo_util import RawVideoExtractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_TrainDataLoader\n",
    "from dataloaders.dataloader_msvd_retrieval import MSVD_DataLoader\n",
    "from dataloaders.dataloader_lsmdc_retrieval import LSMDC_DataLoader\n",
    "from dataloaders.dataloader_activitynet_retrieval import ActivityNet_DataLoader\n",
    "from dataloaders.dataloader_didemo_retrieval import DiDeMo_DataLoader\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from dataloaders.data_dataloaders import DATALOADER_DICT\n",
    "import collections \n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import import_ipynb \n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}\n",
    "_PT_NAME = {\n",
    "    \"RN50\": \"RN50.pt\",\n",
    "    \"RN101\": \"RN101.pt\",\n",
    "    \"RN50x4\": \"RN50x4.pt\",\n",
    "    \"RN50x16\": \"RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"ViT-B-16.pt\",\n",
    "}\n",
    "def _download(url: str, root: str = os.path.expanduser(\"~/.cache/clip\")):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-2]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n",
    "        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '/home/key2317/CLIP4Clip/msvd_data', 'features_path': '/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos', 'max_words': 30, 'feature_framerate': 1, 'max_frames': 16, 'image_resolution': 224, 'frame_order': 0, 'slice_framepos': 0, 'train_frame_order': 0, 'batch_size': 256, 'n_gpu': 7, 'num_thread_reader': 1, 'datatype': 'msvd', 'eval_frame_order': 0, 'batch_size_val': 3500}\n"
     ]
    }
   ],
   "source": [
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":16,\n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":torch.cuda.device_count(), #default :1\n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "})\n",
    "\n",
    "print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '/home/key2317/CLIP4Clip/msvd_data', 'features_path': '/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos', 'max_words': 30, 'feature_framerate': 1, 'max_frames': 100, 'image_resolution': 224, 'frame_order': 0, 'slice_framepos': 0, 'train_frame_order': 0, 'batch_size': 256, 'n_gpu': 7, 'num_thread_reader': 1, 'datatype': 'msvd', 'eval_frame_order': 0, 'batch_size_val': 3500, 'local_rank': 0}\n"
     ]
    }
   ],
   "source": [
    "descriptions = ''\n",
    "cross_model_name = 'cross-base'\n",
    "n_gpu=1\n",
    "cache_dir=\"\"\n",
    "pretrained_clip_name = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "# 회의 get_config는 module_clip 내에 선언되어 있는 CLIP 클래스에 정의되어 있음.\n",
    "clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "CONFIG_NAME = 'cross_config.json'\n",
    "#print(clip_state_dict.keys())\n",
    "action = 'store_true'\n",
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":100,\n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":torch.cuda.device_count(), #default :1\n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "    \"local_rank\":0,\n",
    "})\n",
    "\n",
    "type_vocab_size = 2 \n",
    "task_config = args\n",
    "cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "#print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)\n",
    "print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)\n",
    "cut_top_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "allgather = AllGather.apply\n",
    "def print_shape(target):\n",
    "    print(\"current shape {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeule_clip.py에 있는 내용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck : 연산량을 줄이기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BottleNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ModifiedRsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP4ClipPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP4ClipPreTrainedModel(PreTrainedModel, nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, cross_config, *inputs, **kwargs):\n",
    "        super(CLIP4ClipPreTrainedModel, self).__init__(cross_config)\n",
    "        self.cross_config = cross_config\n",
    "        self.clip = None\n",
    "        self.cross = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cross_model_name, state_dict=None, cache_dir=None, type_vocab_size=2, *inputs, **kwargs):\n",
    "\n",
    "        task_config = None\n",
    "        if \"task_config\" in kwargs.keys():\n",
    "            task_config = kwargs[\"task_config\"]\n",
    "            ###### 20220524 local rank\n",
    "            if not hasattr(task_config, \"local_rank\"):\n",
    "                #args.__dict__[\"max_frames\"]출력하면 100 나옴을 확인\n",
    "                task_config.__dict__[\"local_rank\"] = 0\n",
    "            elif task_config.local_rank == -1:\n",
    "                task_config.local_rank = 0\n",
    "\n",
    "        if state_dict is None: state_dict = {}\n",
    "        pretrained_clip_name = \"ViT-B/32\"\n",
    "        if hasattr(task_config, 'pretrained_clip_name'):\n",
    "            pretrained_clip_name = task_config.pretrained_clip_name\n",
    "        clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "        for key, val in clip_state_dict.items():\n",
    "            new_key = \"clip.\" + key\n",
    "            if new_key not in state_dict:\n",
    "                state_dict[new_key] = val.clone()\n",
    "\n",
    "        cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "\n",
    "        model = cls(cross_config, clip_state_dict, *inputs, **kwargs)\n",
    "\n",
    "        ## ===> Initialization trick [HARD CODE]\n",
    "        if model.linear_patch == \"3d\":\n",
    "            contain_conv2 = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"visual.conv2.weight\") > -1:\n",
    "                    contain_conv2 = True\n",
    "                    break\n",
    "            if contain_conv2 is False and hasattr(model.clip.visual, \"conv2\"):\n",
    "                cp_weight = state_dict[\"clip.visual.conv1.weight\"].clone()\n",
    "                kernel_size = model.clip.visual.conv2.weight.size(2)\n",
    "                conv2_size = model.clip.visual.conv2.weight.size()\n",
    "                conv2_size = list(conv2_size)\n",
    "\n",
    "                left_conv2_size = conv2_size.copy()\n",
    "                right_conv2_size = conv2_size.copy()\n",
    "                left_conv2_size[2] = (kernel_size - 1) // 2\n",
    "                right_conv2_size[2] = kernel_size - 1 - left_conv2_size[2]\n",
    "\n",
    "                left_zeros, right_zeros = None, None\n",
    "                if left_conv2_size[2] > 0:\n",
    "                    left_zeros = torch.zeros(*tuple(left_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "                if right_conv2_size[2] > 0:\n",
    "                    right_zeros = torch.zeros(*tuple(right_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "\n",
    "                cat_list = []\n",
    "                if left_zeros != None: cat_list.append(left_zeros)\n",
    "                cat_list.append(cp_weight.unsqueeze(2))\n",
    "                if right_zeros != None: cat_list.append(right_zeros)\n",
    "                cp_weight = torch.cat(cat_list, dim=2)\n",
    "\n",
    "                state_dict[\"clip.visual.conv2.weight\"] = cp_weight\n",
    "\n",
    "        if model.sim_header == 'tightTransf':\n",
    "            contain_cross = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"cross.transformer\") > -1:\n",
    "                    contain_cross = True\n",
    "                    break\n",
    "            if contain_cross is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"cross.embeddings.position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[\"cross.\"+key] = val.clone()\n",
    "                            continue\n",
    "\n",
    "        if model.sim_header == \"seqLSTM\" or model.sim_header == \"seqTransf\":\n",
    "            contain_frame_position = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"frame_position_embeddings\") > -1:\n",
    "                    contain_frame_position = True\n",
    "                    break\n",
    "            if contain_frame_position is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"frame_position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if model.sim_header == \"seqTransf\" and key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[key.replace(\"transformer.\", \"transformerClip.\")] = val.clone()\n",
    "                            continue\n",
    "        ## <=== End of initialization trick\n",
    "\n",
    "        if state_dict is not None:\n",
    "            model = cls.init_preweight(model, state_dict, task_config=task_config)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_config와 Target_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_log(task_config, info):\n",
    "    if task_config is None or task_config.local_rank == 0:\n",
    "        logger.warning(info)\n",
    "\n",
    "def update_attr(target_name, target_config, target_attr_name, source_config, source_attr_name, default_value=None):\n",
    "    if hasattr(source_config, source_attr_name):\n",
    "        if default_value is None or getattr(source_config, source_attr_name) != default_value:\n",
    "            setattr(target_config, target_attr_name, getattr(source_config, source_attr_name))\n",
    "            show_log(source_config, \"Set {}.{}: {}.\".format(target_name,\n",
    "                                                            target_attr_name, getattr(target_config, target_attr_name)))\n",
    "    return target_config\n",
    "\n",
    "def check_attr(target_name, task_config):\n",
    "    return hasattr(task_config, target_name) and task_config.__dict__[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALoader 생성하여 video의 shape(6차원)을 확인 \n",
    "video shape : \n",
    " - Pair : 1\n",
    " - max_frame : 100 \n",
    " - batch : 1 \n",
    " - Channel : 3 \n",
    " - H : 224\n",
    " - W : 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_msvd_train(args,tokenizer):\n",
    "    msvd_dataset=MSVD_DataLoader(\n",
    "        subset = \"train\",\n",
    "        data_path = args.data_path,\n",
    "        features_path = args.features_path,\n",
    "        max_words = args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer = tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order = args.train_frame_order, \n",
    "        slice_framepos = args.slice_framepos\n",
    "    )\n",
    "\n",
    "    #train_sampler = torch.utils.data.distributed.DistributedSampler(msvd_dataset)\n",
    "    train_sampler = 0\n",
    "    dataloader = DataLoader(\n",
    "        msvd_dataset,\n",
    "        batch_size = args.batch_size // args.n_gpu, \n",
    "        num_workers = args.num_thread_reader,\n",
    "        pin_memory=False, \n",
    "        shuffle = (train_sampler is None), \n",
    "        sampler = train_sampler, \n",
    "        drop_last=True,\n",
    "\n",
    "        \n",
    "    )\n",
    "\n",
    "    return msvd_dataset, dataloader, len(msvd_dataset),train_sampler\n",
    "\n",
    "def dataloader_msvd_test(args, tokenizer, subset=\"test\"):\n",
    "    msvd_testset = MSVD_DataLoader(\n",
    "        subset=subset,\n",
    "        data_path=args.data_path,\n",
    "        features_path=args.features_path,\n",
    "        max_words=args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer=tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order=args.eval_frame_order,\n",
    "        slice_framepos=args.slice_framepos,\n",
    "    )\n",
    "    dataloader_msvd = DataLoader(\n",
    "        msvd_testset,\n",
    "        batch_size=args.batch_size_val,\n",
    "        num_workers=args.num_thread_reader,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return msvd_testset, dataloader_msvd, len(msvd_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video number: 1200\n",
      "Total Paire: 48774\n",
      "For test, sentence number: 27763\n",
      "For test, video number: 670\n",
      "Video number: 670\n",
      "Total Paire: 27763\n"
     ]
    }
   ],
   "source": [
    "msvd_dataset,train_dataloader,len_of_msvdtrain,train_sampler = dataloader_msvd_train(args,tokenizer)\n",
    "msvd_testset, test_dataloader, len_of_msvdtest = dataloader_msvd_test(args,tokenizer,subset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_PATH와  FEATURE_PATH를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PATH : /home/key2317/CLIP4Clip/msvd_data\n",
      "FEATURE PATH  /home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA PATH :\",DATA_PATH)\n",
    "print(\"FEATURE PATH \",FEATURES_PATH)\n",
    "video_id_path_dict = {}\n",
    "video_id_path_dict[\"train\"] = os.path.join(DATA_PATH, \"train_list.txt\")\n",
    "video_id_path_dict[\"val\"] = os.path.join(DATA_PATH, \"val_list.txt\")\n",
    "video_id_path_dict[\"test\"] = os.path.join(DATA_PATH, \"test_list.txt\")\n",
    "caption_file = os.path.join(DATA_PATH, \"raw-captions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "captions : video_id와 대사로 이루어진 dict\n",
    "\n",
    "video_ids : video_id를 모아놓은 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caption_file,'rb') as f:\n",
    "    captions = pickle.load(f)\n",
    "\n",
    "with open(video_id_path_dict[\"train\"], 'r') as fp:\n",
    "    video_ids = [itm.strip() for itm in fp.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video_dict : 파일과 video_id로 이루어진 dict \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {} \n",
    "for root, dub_dir,video_files in os.walk(args.features_path):\n",
    "    for video_file in video_files:\n",
    "        video_id_ = \".\".join(video_file.split(\".\")[:-1])\n",
    "        if video_id_ not in video_ids:\n",
    "            continue\n",
    "        file_path_ = os.path.join(root,video_file)\n",
    "        video_dict[video_id_] = file_path_ \n",
    "\n",
    "#print(video_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences_dict : key : 인덱스 / value : (video_id, 문장) 으로 이루어진 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48774\n"
     ]
    }
   ],
   "source": [
    "sentences_dict = {} \n",
    "cut_off_points=[] \n",
    "for video_id in video_ids:\n",
    "    assert video_id in captions \n",
    "    for cap in captions[video_id]:\n",
    "        cap_txt = \" \".join(cap)\n",
    "        sentences_dict[len(sentences_dict)] = (video_id,cap_txt)\n",
    "    cut_off_points.append(len(sentences_dict))\n",
    "#print(sentences_dict)\n",
    "#print(cut_off_points)\n",
    "print(len(sentences_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msvd_dataset에서 getitem을 할 때, \n",
    " - get_text의 shape\n",
    " - get_video의 shape\n",
    "\n",
    "를 확인하는 작업입니다.\n",
    "\n",
    "dataset에서 _getitem_을 통해 인덱스 하나에 대하여 video와 text를 각각 꺼내옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4wsuPCjDBc_5_15\n",
      "a chipmunk is eating\n",
      "['-4wsuPCjDBc_5_15']\n"
     ]
    }
   ],
   "source": [
    "idx = 1 \n",
    "video_id,caption = sentences_dict[idx]\n",
    "print(video_id)\n",
    "print(caption)\n",
    "pairs_text, pairs_mask, pairs_segment, choice_video_ids  = msvd_dataset._get_text(video_id,caption)\n",
    "\n",
    "print(choice_video_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_video_data_clip shape : torch.Size([11, 3, 224, 224])\n",
      "raw_video_slice shape : torch.Size([11, 1, 3, 224, 224])\n",
      "(1, 100, 1, 3, 224, 224)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "video, video_mask = msvd_dataset._get_rawvideo(choice_video_ids)\n",
    "print(video.shape)\n",
    "print(video_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_video_data_clip shape : torch.Size([11, 3, 224, 224])\n",
      "raw_video_slice shape : torch.Size([11, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "pairs_text,pairs_mask,pairs_segment,video,video_mask = msvd_dataset.__getitem__(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self, spacial_dim: int, embed_dim : int, num_heads : int, output_dim : int = None):\n",
    "        super(AttentionPool2d,self).__init__()\n",
    "        #spacial_dim의 제곱 + 1 , embed_dim \n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 +1 ,embed_dim) / embed_dim ** 0.5)\n",
    "        print(self.positional_embedding.shape)\n",
    "        # Key, Query, Value\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(embed_dim,output_dim or embed_dim)\n",
    "        self.num_heads = num_heads \n",
    "\n",
    "    def forward(self,x):\n",
    "        ###################### 차원 한번 줄임 ######################\n",
    "        x = x.reshape(x.shape[0],x.shape[1],x.shape[2] * x.shape[3]).permute(2,0,1) # NCHW -> (HW)NC\n",
    "        print('permuted x shape : {}'.format(x.shape))\n",
    "        x = torch.cat([x.mean(dim=0,keepdim=True),x],dim=0) #(HW+1)NC\n",
    "        print('torch cat x shape :',x.shape) #(224 * 224 +1 , 1, 3)\n",
    "        x = x + self.positional_embedding[:,None, :].to(x.dtype) # (HW+1)NC\n",
    "        print(\"added with positional_embedding shape :\",x.shape)\n",
    "        ## multi head attention 수행\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query = x, key = x, value = x, \n",
    "            \n",
    "            embed_dim_to_check = x.shape[-1],\n",
    "            num_heads = self.num_heads, \n",
    "\n",
    "            q_proj_weight = self.q_proj.weight, #nn.Linear(embed_dim,embed_dim) 의 weight\n",
    "            k_proj_weight = self.k_proj.weight, #Linear \n",
    "            v_proj_weight = self.v_proj.weight, \n",
    "            in_proj_weight = None, \n",
    "            in_proj_bias = torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), \n",
    "            bias_k = None,\n",
    "            bias_v = None, \n",
    "            add_zero_attn = False, \n",
    "            dropout_p= 0, \n",
    "            out_proj_weight=self.c_proj.weight, \n",
    "            out_proj_bias= self.c_proj.bias, \n",
    "            use_separate_proj_weight=True, \n",
    "            training = self.training, \n",
    "            need_weights = False\n",
    "        )\n",
    "\n",
    "        return x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "\n",
    "transformer_heads = transformer_width // 64\n",
    "\n",
    "#transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\"))) #12\n",
    "\n",
    "transformer_layers = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttentionPool2d의 spec \n",
    " - spacial_dim : 224 \n",
    " - embed_dim : 3 \n",
    " - num_heads : 3 \n",
    " - output_dim : 512 \n",
    " - embedding 의 shape : (spacial_dim) * (spacial_dim) , 1 , 3 \n",
    "\n",
    " - input x의 shape : (N : 1, C : 3, H : 224, W : 224)\n",
    " - output x[0]의 shape : 1 , output_dim 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "\n",
    "    def forward(self,x : torch.Tensor):\n",
    "        orig_type = x.dtype \n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReisudalAttentionBlock : 크게 다를 것은 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    # d_model이 Transformer에서 넘어온 width와 같음.\n",
    "    # # d_model == width  \n",
    "    def __init__(self, d_model : int, n_head : int, attn_mask = None):\n",
    "        super(ResidualAttentionBlock,self).__init__()\n",
    "        \n",
    "        ##### attention Block\n",
    "        self.attn = nn.MultiheadAttention(d_model,n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\",nn.Linear(d_model,d_model*4)),\n",
    "            (\"gelu\",QuickGELU()),\n",
    "            (\"c_proj\",nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask \n",
    "    \n",
    "    def attention(self,x:torch.Tensor):\n",
    "        attn_mask_ = self.attn_mask \n",
    "        if self.attn_mask is not None and hasattr(self.attn_mask, '__call__'):\n",
    "            attn_mask_ = self.attn_mask(x.size(0))\n",
    "        attn_mask_ = attn_mask_.to(dtype=x.dtype,device = x.device) if attn_mask_ is not None else None \n",
    "        return self.attn(x,x,x,need_weights=False, attn_mask = attn_mask_)[0]\n",
    "    \n",
    "    def forward(self,x_tuple : tuple):\n",
    "        x,video_frame = x_tuple  \n",
    "        #print(\"Residualattention video_frame :\",video_frame)\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return (x,video_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width :int, layers :int , heads :int, attn_mask = None):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.width = width \n",
    "        self.layers = layers \n",
    "        # layer의 숫자만큼 Residualattention을 진행\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width,heads,attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, video_frame = -1):\n",
    "        return self.resblocks((x,video_frame))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisionTransformer : \n",
    "\n",
    "transformer_width : clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "\n",
    "transformer_heads : transformer_width// 64 64등분!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self,input_resolution : int, patch_size : int, width : int, layers : int, heads : int, output_dim : int, linear_patch : str = '2d',):\n",
    "        super(VisualTransformer,self).__init__()\n",
    "        self.input_resolution = input_resolution \n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        ##### 2D일 때에는 Conv1d\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels = width, kernel_size = patch_size,stride=patch_size,bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size)**2 +1, width)) \n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "\n",
    "        #Transformer 인자 1 : width / 인자 2 : layers / 인자 3 : heads \n",
    "        self.transformer = Transformer(width,layers,heads)\n",
    "\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width,output_dim))   \n",
    "\n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch \n",
    "        if self.linear_patch == '3d':\n",
    "            #### 3D일 때에는 Conv2d\n",
    "            self.conv2 = nn.Conv3d(in_channels=3, out_channels=width, kernel_size = (3,patch_size,patch_size), \n",
    "            stride = (1,patch_size,patch_size),padding = (1,0,0),bias= False)\n",
    "    # Task 2 !!! \n",
    "    def forward(self, x: torch.Tensor, video_frame=-1):\n",
    "            ####Conv3D AE 20220526 #####\n",
    "            #x_3d shape : \n",
    "            x_3d = x.reshape(-1, video_frame, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            print(\"1111 x_3d shape :\",x_3d.shape)\n",
    "            if self.linear_patch == '3d':\n",
    "                assert video_frame != -1\n",
    "                x_3d = x.reshape(-1, video_frame, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "                print(\"2222 x_3d shape :\",x_3d.shape)\n",
    "                x_3d = x_3d.permute(0, 2, 1, 3, 4) #\n",
    "                print(\"3333 x_3d shape :\",x_3d.shape)\n",
    "                #x_3d = x_3d.permute()\n",
    "                x_3d = self.conv2(x_3d)     # shape = [*, width, frame, grid, grid]\n",
    "                print(\"4444 x_3d shape :\",x_3d.shape)\n",
    "                x_3d = x_3d.permute(0, 2, 1, 3, 4)      # shape = [*, frame, width, grid, grid]\n",
    "                print(\"5555 x_3d shape :\",x_3d.shape)\n",
    "                x = x_3d.reshape(-1, x_3d.shape[-3], x_3d.shape[-2], x_3d.shape[-1]).contiguous() # shape = [*, width, grid, grid]\n",
    "                print(\"666 x_3d shape :\",x_3d.shape)\n",
    "            else:\n",
    "                x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "\n",
    "            #################################### 20220607 여기까지는 구현 성공함 ##################################\n",
    "            x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "            x = x + self.positional_embedding.to(x.dtype) \n",
    "            x = self.ln_pre(x)\n",
    "            print(\"<<<< x shape : >>>>>\",x.shape)\n",
    "            ################# TAE!!! #################### \n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            ############### 원래 기존으로 가 #######################\n",
    "            x = self.transformer(x, video_frame=video_frame)\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "            # Move the three lines below to `encode_image` for entire hidden sequence\n",
    "            # x = self.ln_post(x[:, 0, :])\n",
    "            # if self.proj is not None:\n",
    "            #     x = x @ self.proj\n",
    "\n",
    "            return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP 해부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CLIP 초기화에 필요한 Vision parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vision_layers = len([k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "image_resolution = vision_patch_size * grid_size # 32 x 7 \n",
    "vision_heads = vision_width //64\n",
    "linear_patch = '2d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CLIP 초기화에 필요한 text parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "transformer_heads = transformer_width // 64\n",
    "#transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\"))) #12\n",
    "transformer_layers = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nn.Embedding(vocab_size,transformer_width)\n",
    "positional_embedding = nn.Parameter(torch.empty(context_length,transformer_width))\n",
    "ln_final = LayerNorm(transformer_width)\n",
    "text_projection = nn.Parameter(torch.empty(transformer_width,embed_dim))\n",
    "logit_scale = nn.Parameter(torch.ones([]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CLIP 내부에 있는 모든 ViT, text transformer 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### visual transformer\n",
    "visual = VisualTransformer(\n",
    "    input_resolution = image_resolution, \n",
    "    patch_size=vision_patch_size,\n",
    "    width=vision_width,\n",
    "    layers=vision_layers,\n",
    "    heads=vision_heads,\n",
    "    output_dim=embed_dim,\n",
    "    linear_patch=linear_patch    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_attention_mask(context_length):\n",
    "# lazily create causal attention mask, with full attention between the vision tokens\n",
    "# pytorch uses additive attention mask; fill with -inf\n",
    "    mask = torch.zeros(context_length, context_length)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    return mask\n",
    "\n",
    "\n",
    "############ Text transformer\n",
    "transformer = Transformer(\n",
    "    width=transformer_width,\n",
    "    layers=transformer_layers,\n",
    "    heads=transformer_heads,\n",
    "    attn_mask=build_attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text와 image를 Encode하는 함수, 그리고 이를 받아서 forward하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def dtype():\n",
    "    return visual.conv1.weight.dtype\n",
    "\n",
    "def encode_image(image, return_hidden=False, video_frame=-1):\n",
    "    hidden = visual(image.type(dtype), video_frame=video_frame)\n",
    "    hidden = visual.ln_post(hidden) @ visual.proj\n",
    "\n",
    "    x = hidden[:, 0, :]\n",
    "\n",
    "    if return_hidden:\n",
    "        return x, hidden\n",
    "\n",
    "    return x\n",
    "\n",
    "def encode_text(text, return_hidden=False):\n",
    "    x = token_embedding(text).type(dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "    pos_emd = positional_embedding[:x.size(1), :].type(dtype)\n",
    "    x = x + pos_emd\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    x = transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "    hidden = ln_final(x).type(dtype) @ text_projection\n",
    "\n",
    "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "    x = hidden[torch.arange(hidden.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "    if return_hidden:\n",
    "        return x, hidden\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward(image, text):\n",
    "    # cae = CAE() # encoder : 차원 축소, autoencoder : encoder + decoder\n",
    "    # encoded, decoded = encoder(image)\n",
    "\n",
    "    # image_features = encoded_image(encoded)\n",
    "    # text_features = encoded_text(text)\n",
    "\n",
    "    # reconstruction_error = mse_loss(encoded, decoded)\n",
    "    # return logits_per_image, logits_per_text, reconstruction_error\n",
    "    # (t2v + v2tv)/2 + reconstruction_error, or (total loss) / 3\n",
    "    \n",
    "    ####################### 20220531 CAE #####################\n",
    "    image_features = encode_image(image)\n",
    "    text_features = encode_text(text)\n",
    "\n",
    "    # normalized features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    # T x C x H X W\n",
    "    def __init__(self, frames, ch, width, height):\n",
    "        super(CAE, self).__init__()\n",
    "        self.frames = frames\n",
    "        self.ch = ch\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        def EncoderLayer(in_channel, out_channel, pooling_size=None):\n",
    "            network = []\n",
    "            #network += [TImeDistributed(nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            \n",
    "            network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            if pooling_size is not None:\n",
    "                network += [nn.MaxPool3d(kernel_size=pooling_size)]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        def DecoderLayer(in_channel, out_channel, size=None):\n",
    "            network = []\n",
    "            if size is None:\n",
    "                network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            else:\n",
    "                network += [nn.ConvTranspose3d(in_channel, out_channel, kernel_size=size, padding=0, stride=size)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        self.encoder1 = EncoderLayer(3, 32, pooling_size=2)\n",
    "        self.encoder2 = EncoderLayer(32, 48, pooling_size=2)\n",
    "        self.encoder3 = EncoderLayer(48, 64)\n",
    "        ##################################\n",
    "        \n",
    "\n",
    "        ##################################\n",
    "        self.decoder1 = DecoderLayer(64, 48)\n",
    "        self.decoder2 = DecoderLayer(48, 32, size=2)\n",
    "        self.decoder3 = DecoderLayer(32, 3, size=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        #encoded = self.encoder3(self.encoder2(self.encoder1(x)))\n",
    "        x = self.encoder1(x)\n",
    "        #print(\"encoder1 통과했을 때 x shape : \",x.shape)\n",
    "        x = self.encoder2(x)\n",
    "        #print(\"encoder2 통과했을 때 x shape :\",x.shape)\n",
    "        encoded = self.encoder3(x)\n",
    "        #torch.Size([1, 64, 56, 56, 4])\n",
    "        res_encoded = encoded.permute(0,4,2,3,1) # 1, 4, 56, 56, 64\n",
    "        res_encoded = res_encoded.permute(0,1,4,3,2)\n",
    "        #print('마지막 encoded shape :',encoded.shape)\n",
    "        decoded = self.decoder3(self.decoder2(self.decoder1(encoded)))\n",
    "        \n",
    "        decoded = self.sigmoid(decoded)\n",
    "        #print(\"decodered x shape \",decoded.shape)\n",
    "        return res_encoded, decoded\n",
    "    def compute_loss(self, x, y):\n",
    "        mse = nn.MSELoss()\n",
    "        loss = mse(x, y)\n",
    "        return loss, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "# encoded shape : torch.Size([10, 64, 56, 56, 3])\n",
    "# decoded shape : torch.Size([10, 3, 224, 224, 12])\n",
    "# image shape toward VIT -> ch : 64, h : 56, w : 56, f : 3 \n",
    "# (not same as oirignal shape (3, 224, 224, 12) -> have to solove the different shape\n",
    "# otherwise, (B, C, H, W ,F) = (B, 3, 224, 224, 1) (if not solved, jh tries this method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = CAE(frames=16,ch=3,width=image_resolution,height=image_resolution)\n",
    "video = torch.randn(1,3,224,224,16)\n",
    "#video = torch.randn(1,16,3,224,224)\n",
    "res_encoded, decoded = set.forward(video)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP4Clip에 정의된 get_visual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 input_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 # vision linear of patch\n",
    "                 linear_patch: str = '2d',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=input_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisualTransformer(\n",
    "                input_resolution=input_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim,\n",
    "                linear_patch=linear_patch\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config(pretrained_clip_name=\"ViT-B/32\"):\n",
    "        model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"ViT-B-32.pt\")\n",
    "        if pretrained_clip_name in _MODELS and pretrained_clip_name in _PT_NAME:\n",
    "            model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), _PT_NAME[pretrained_clip_name])\n",
    "\n",
    "        if pretrained_clip_name in [\"ViT-B/32\", \"ViT-B/16\"] and os.path.exists(model_path):\n",
    "            pass\n",
    "        else:\n",
    "            if pretrained_clip_name in _MODELS:\n",
    "                model_path = _download(_MODELS[pretrained_clip_name])\n",
    "            elif os.path.isfile(pretrained_clip_name):\n",
    "                model_path = pretrained_clip_name\n",
    "            else:\n",
    "                raise RuntimeError(f\"Model {pretrained_clip_name} not found; available models = {available_models()}\")\n",
    "\n",
    "        try:\n",
    "            # loading JIT archive\n",
    "            model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "            state_dict = model.state_dict()\n",
    "        except RuntimeError:\n",
    "            state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def build_attention_mask(self, context_length):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.zeros(context_length, context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "    \n",
    "    # input : video(4D) video_frame : bs * ts\n",
    "    def encode_input(self, input, return_hidden=False, video_frame=-1):\n",
    "\n",
    "        hidden = self.visual(input.type(self.dtype), video_frame=video_frame)\n",
    "        hidden = self.visual.ln_post(hidden) @ self.visual.proj\n",
    "\n",
    "        x = hidden[:, 0, :]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode_text(self, text, return_hidden=False):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        pos_emd = self.positional_embedding[:x.size(1), :].type(self.dtype)\n",
    "        x = x + pos_emd\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        hidden = self.ln_final(x).type(self.dtype) @ self.text_projection\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = hidden[torch.arange(hidden.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, input, text):\n",
    "        input_features = self.encode_input(input)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        input_features = input_features / input_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_input = logit_scale * input_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ input_features.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_input, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP(\n",
    "            embed_dim,\n",
    "            image_resolution, vision_layers-cut_top_layer, vision_width, vision_patch_size,\n",
    "            context_length, vocab_size, transformer_width, transformer_heads, transformer_layers-cut_top_layer,\n",
    "            linear_patch=linear_patch\n",
    "        ).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisualTransformer(\n",
    "input_resolution = image_resolution, \n",
    "patch_size = vision_patch_size, \n",
    "width = vision_width, \n",
    "layers = vision_layers,\n",
    "heads= vision_heads,\n",
    "output_dim=embed_dim, \n",
    "linear_patch=linear_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image resolution : 224\n",
      "vision_patch_size : 32\n",
      "vision_width : 768\n",
      "vision_layers : 12\n",
      "vision_heads : 12\n",
      "vision_heads : 12\n",
      "output_dim : 512\n",
      "linear patch : 2d\n"
     ]
    }
   ],
   "source": [
    "print(\"image resolution :\",image_resolution)\n",
    "print('vision_patch_size :',vision_patch_size)\n",
    "print('vision_width :',vision_width)\n",
    "print(\"vision_layers :\",vision_layers)\n",
    "print(\"vision_heads :\",vision_heads)\n",
    "print(\"vision_heads :\",vision_heads)\n",
    "print(\"output_dim :\",embed_dim)\n",
    "print(\"linear patch :\",linear_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisionAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUTOENCODER(nn.Module):\n",
    "    def __init__(self, channel, width : int, height : int, linear_patch='3d'):\n",
    "        super(AUTOENCODER,self).__init__()\n",
    "        self.width  = width \n",
    "        self.height = height \n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch\n",
    "\n",
    "        def EncoderLayer(in_channel,out_channel,pooling_size=None):\n",
    "            network=[] \n",
    "\n",
    "            network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            if pooling_size is not None:\n",
    "                network += [nn.MaxPool3d(kernel_size=pooling_size)]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "\n",
    "        def DecoderLayer(in_channel,out_channel,size=None):\n",
    "            network = [] \n",
    "            if size is None:\n",
    "                network +=[nn.Conv3d(in_channel,out_channel,kernel_size=3,padding=1)]\n",
    "            else:\n",
    "                network +=[nn.ConvTranspose3d(in_channel,out_channel,kernel_size=size,padding=0,stride=size)]\n",
    "            network +=[nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        \n",
    "        self.encoder1 = EncoderLayer(channel, 32, pooling_size=2)\n",
    "        self.encoder2 = EncoderLayer(32, 48, pooling_size=2)\n",
    "        self.encoder3 = EncoderLayer(48, 64)\n",
    "        self.decoder1 = DecoderLayer(64, 48)\n",
    "        self.decoder2 = DecoderLayer(48, 32, size=2)\n",
    "        self.decoder3 = DecoderLayer(32, channel, size=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # case 20220607 : self.encoder = EncoderLayer(48, 768) & not used conv2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder1(x)\n",
    "        x = self.encoder2(x)\n",
    "        encoded = self.encoder3(x)\n",
    "        decoded = self.decoder3(self.decoder2(self.decoder1(encoded))) \n",
    "        decoded = self.sigmoid(decoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "    def compute_loss(self, x, y):\n",
    "        mse = nn.MSELoss()\n",
    "        loss = mse(x, y)\n",
    "        return loss, loss.item()\n",
    "\n",
    "class VAeT(nn.Module):\n",
    "    def __init__(self,input_resolution : int, patch_size : int, width : int, layers : int, heads : int, output_dim : int, linear_patch :str = '2d'):\n",
    "        super(VAeT, self).__init__()\n",
    "\n",
    "        self.AE = AUTOENCODER(channel=3,width=image_resolution,height=image_resolution)\n",
    "\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        ### 2d일 때에는 Conv1d \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=width,kernel_size = patch_size,stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5 \n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(65, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "        \n",
    "        \n",
    "        self.transformer = Transformer(width,layers,heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale *torch.randn(width,output_dim))\n",
    "\n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch \n",
    "\n",
    "        if self.linear_patch == '3d':\n",
    "            self.conv2 = nn.Conv3d(in_channels=64,out_channels=vision_width,kernel_size=(7,7,1),\n",
    "                stride = (7,7,1),padding=(0,1,0),bias=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor,video_frame=-1):\n",
    "        video_frame = x.shape[1] # video frame\n",
    "        \n",
    "        encoded, decoded = self.AE(x)\n",
    "\n",
    "        # for VIT\n",
    "        x = self.conv2(encoded)\n",
    "        x = x.permute(0,4,1,3,2)\n",
    "        x = x.reshape(-1,x.shape[-3],x.shape[-2]*x.shape[-1]).contiguous()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype) \n",
    "        x = self.ln_pre(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.transformer(x,video_frame=3)\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        return x, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded and VAeT res shape : torch.Size([3, 65, 768])\n"
     ]
    }
   ],
   "source": [
    "model = VAeT(input_resolution=image_resolution,patch_size=vision_patch_size,width=vision_width,\n",
    "layers = vision_layers,heads=vision_heads,output_dim=embed_dim,linear_patch='3d')\n",
    "\n",
    "sample_video = torch.randn(1,3,224,224,12) # NLD x channel x width x height x frame \n",
    "# res_encoded,decoded = test.forward(sample_video)\n",
    "\n",
    "res_encoded, decoded = model(sample_video)\n",
    "print(\"encoded and VAeT res shape :\",res_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eff65c0def8c12c365a8ff92cc515e5fbadd507d935c66604f35c789f0fc4099"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CLIP4Clip': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
