{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/key2317/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm, notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        return (data, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (str, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/key2317/transformer.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=83'>84</a>\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe[:x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), :]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=84'>85</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x         \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=86'>87</a>\u001b[0m pe \u001b[39m=\u001b[39m PositionalEncoding(\u001b[39m128\u001b[39;49m, \u001b[39m5000\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=87'>88</a>\u001b[0m token_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m128\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=88'>89</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpe encoding shape : \u001b[39m\u001b[39m\"\u001b[39m , pe\u001b[39m.\u001b[39mencoding\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/home/key2317/transformer.ipynb Cell 3'\u001b[0m in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, d_model, dropout, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_model, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_len\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=67'>68</a>\u001b[0m     \u001b[39msuper\u001b[39m(PositionalEncoding, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=69'>70</a>\u001b[0m     pe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(max_len, d_model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=70'>71</a>\u001b[0m     position \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, max_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/transformer.ipynb#ch0000002vscode-remote?line=72'>73</a>\u001b[0m     div_term \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, d_model, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39mlog(\u001b[39m10000.0\u001b[39m) \u001b[39m/\u001b[39m d_model))\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (str, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None, e=1e-6):\n",
    "        batch_size, head, length, d_tensor = k.size()       # ex) (2, 4, 100, 32)\n",
    "        k_t = k.view(batch_size, head, d_tensor, length)    # (2, 4, 32, 100)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -e)\n",
    "        score = self.softmax(score)\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def split(self, t):\n",
    "        batch_size, length, d_model = t.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        t = t.view(batch_size, self.n_head, length, d_tensor)\n",
    "        return t\n",
    "\n",
    "    def concat(self, t):\n",
    "        batch_size, head, length, d_tensor = t.size()\n",
    "        d_model = d_tensor * head\n",
    "        t = t.view(batch_size, length, d_model)\n",
    "        return t\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape, self.pe[:x.size(1), :].squeeze(1).shape)\n",
    "        x = x + self.pe[:x.size(1), :].squeeze(1)\n",
    "        return x         \n",
    "        \n",
    "# pe = PositionalEncoding(128, 5000, 'cpu')\n",
    "# token_embedding = torch.rand(size=(1, 100, 128))\n",
    "# print(\"pe encoding shape : \" , pe.encoding.shape)\n",
    "# result = pe(token_embedding)\n",
    "# print(\"result shape : \" , result.shape)\n",
    "\n",
    "# pe = PositionalEncoding(128, 0.1, 5000)\n",
    "# token_embedding = torch.rand(size=(1, 100, 128))\n",
    "# result = pe(token_embedding)\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        x = self.norm1(x + _x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        x = self.norm2(_x + x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, \n",
    "                        drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        x = x + self.pe(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# inputs = torch.rand(size=(1, 100, 128))\n",
    "# encoder = Encoder(128, 256, 4, 1, 0.1)\n",
    "# result = encoder(inputs, None)\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.ones(size, size).triu(diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask):\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        x = self.norm1(x+_x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        x = self.norm3(x+_x)\n",
    "        x = self.dropout3(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, n_head=n_head, drop_prob=drop_prob) for _ in range(n_layers)])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, trg, src, trg_mask, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "        \n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, n_head, n_layers, drop_prob)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, n_head, n_layers, drop_prob)\n",
    "        self.conv1x1 = nn.Conv1d(in_channels=input_size, out_channels=d_model, kernel_size=1, stride=1)\n",
    "\n",
    "    def make_dataset(self, x):\n",
    "        enc_seq_len = int(x.shape[1] * 0.8)\n",
    "        dec_seq_len = int(x.shape[1] * 0.2)\n",
    "        output_sequence_length = int(x.shape[1] * 0.2)\n",
    "\n",
    "        enc_input = x[:, :enc_seq_len, :]\n",
    "        dec_input = enc_input[:, -dec_seq_len:, :]\n",
    "        dec_output = x[:,-output_sequence_length:, :]\n",
    "\n",
    "        return enc_input, dec_input, dec_output    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        print(type(inputs))\n",
    "        inputs = torch.permute(inputs, dims=(0, 2, 1))\n",
    "        print(type(inputs))\n",
    "        x = self.conv1x1(inputs)\n",
    "        x = torch.permute(x, dims=(0, 2, 1))\n",
    "        print(type(x))\n",
    "        enc_input, dec_input, dec_output = self.make_dataset(x)\n",
    "        print(type(enc_input))\n",
    "\n",
    "        enc = self.encoder(enc_input, None)\n",
    "        dec = self.decoder(dec_input, enc, create_look_ahead_mask(dec_input.size(1)))\n",
    "\n",
    "        return dec, dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input, prediction):\n",
    "    mse = nn.MSELoss()\n",
    "    loss_mse = mse(prediction, input)\n",
    "    return loss_mse, loss_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, dataloader):\n",
    "\n",
    "#     loss_epoch = []\n",
    "#     model.train()\n",
    "\n",
    "#     for index_batch, (inputs, labels) in tqdm(enumerate(dataloader)):\n",
    "        \n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         predicted, true = model(inputs)\n",
    "#         loss, loss_value = compute_loss(true, predicted)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loss_epoch.append(loss_value)\n",
    "\n",
    "#     loss_mean_epoch = np.mean(loss_epoch)\n",
    "#     loss_std_epoch = np.mean(loss_epoch)\n",
    "#     loss = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "losses_mean = np.zeros(epochs)\n",
    "losses_std = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x trian shape :  torch.Size([27815, 200, 3])\n",
      "y train shape :  torch.Size([27815, 6])\n"
     ]
    }
   ],
   "source": [
    "from wisdm import wisdm\n",
    "x_train, y_train = wisdm.create_wisdm()\n",
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "print(\"x trian shape : \", x_train.shape)\n",
    "print(\"y train shape : \", y_train.shape)\n",
    "\n",
    "dataset_train = dataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "transformer = Transformer(input_size=x_train.shape[2], d_model=128, ffn_hidden=256, n_head=4, n_layers=2, drop_prob=0.1)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tolom/anaconda3/envs/ajh/lib/python3.8/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/home/tolom/anaconda3/envs/ajh/lib/python3.8/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A10 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA A10 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/home/tolom/anaconda3/envs/ajh/lib/python3.8/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA RTX A5000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA RTX A5000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "    transformer.train()\n",
    "\n",
    "    for index_batch, (inputs, labels) in tqdm(enumerate(dataloader_train)):\n",
    "        print(index_batch)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predicted, true = transformer(inputs)\n",
    "        loss, loss_value = compute_loss(true, predicted)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch.append(loss_value)\n",
    "\n",
    "    loss_mean_epoch = np.mean(loss_epoch)\n",
    "    loss_std_epoch = np.mean(loss_epoch)\n",
    "    loss = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
    "\n",
    "    loss_train = train(transformer, dataloader_train)\n",
    "    losses_mean[i] = loss_train['mean']\n",
    "    losses_std[i] = loss_train['std']\n",
    "\n",
    "    print('epoch : ', i, 'loss_train : ', losses_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/key2317/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eff65c0def8c12c365a8ff92cc515e5fbadd507d935c66604f35c789f0fc4099"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CLIP4Clip': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
