{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modules.until_module import PreTrainedModel, AllGather, CrossEn\n",
    "from modules.module_cross import CrossModel, CrossConfig, Transformer as TransformerClip\n",
    "\n",
    "from modules.module_clip import CLIP, convert_weights\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataloaders.rawvideo_util import RawVideoExtractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_TrainDataLoader\n",
    "from dataloaders.dataloader_msvd_retrieval import MSVD_DataLoader\n",
    "from dataloaders.dataloader_lsmdc_retrieval import LSMDC_DataLoader\n",
    "from dataloaders.dataloader_activitynet_retrieval import ActivityNet_DataLoader\n",
    "from dataloaders.dataloader_didemo_retrieval import DiDeMo_DataLoader\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from dataloaders.data_dataloaders import DATALOADER_DICT\n",
    "import collections \n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import import_ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}\n",
    "_PT_NAME = {\n",
    "    \"RN50\": \"RN50.pt\",\n",
    "    \"RN101\": \"RN101.pt\",\n",
    "    \"RN50x4\": \"RN50x4.pt\",\n",
    "    \"RN50x16\": \"RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"ViT-B-16.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '/home/key2317/CLIP4Clip/msvd_data', 'features_path': '/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos', 'max_words': 30, 'feature_framerate': 1, 'max_frames': 100, 'image_resolution': 224, 'frame_order': 0, 'slice_framepos': 0, 'train_frame_order': 0, 'batch_size': 256, 'n_gpu': 6, 'num_thread_reader': 1, 'datatype': 'msvd', 'eval_frame_order': 0, 'batch_size_val': 3500}\n"
     ]
    }
   ],
   "source": [
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":100,\n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":torch.cuda.device_count(), #default :1\n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "})\n",
    "\n",
    "print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '/home/key2317/CLIP4Clip/msvd_data', 'features_path': '/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos', 'max_words': 30, 'feature_framerate': 1, 'max_frames': 100, 'image_resolution': 224, 'frame_order': 0, 'slice_framepos': 0, 'train_frame_order': 0, 'batch_size': 256, 'n_gpu': 6, 'num_thread_reader': 1, 'datatype': 'msvd', 'eval_frame_order': 0, 'batch_size_val': 3500, 'local_rank': 0}\n"
     ]
    }
   ],
   "source": [
    "descriptions = ''\n",
    "cross_model_name = 'cross-base'\n",
    "n_gpu=1\n",
    "cache_dir=\"\"\n",
    "pretrained_clip_name = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "# 회의 get_config는 module_clip 내에 선언되어 있는 CLIP 클래스에 정의되어 있음.\n",
    "clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "CONFIG_NAME = 'cross_config.json'\n",
    "#print(clip_state_dict.keys())\n",
    "action = 'store_true'\n",
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":100,\n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":torch.cuda.device_count(), #default :1\n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "    \"local_rank\":0,\n",
    "})\n",
    "\n",
    "type_vocab_size = 2 \n",
    "task_config = args\n",
    "cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "#print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)\n",
    "print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "allgather = AllGather.apply\n",
    "def print_shape(target):\n",
    "    print(\"current shape {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeule_clip.py에 있는 내용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck : 연산량을 줄이기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4 \n",
    "\n",
    "    def __init__(self,inplanes, planes, stride = 1):\n",
    "        super(Bottleneck,self).__init__() \n",
    "\n",
    "        # all conv layer have stride 1. \n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias= False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        #print_shape(self.bn1)\n",
    "\n",
    "        self.conv2 =nn.Conv2d(planes,planes,3,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        #print_shape(self.bn2)\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity() \n",
    "\n",
    "        #print_shape(self.avgpool)\n",
    "        self.conv3 = nn.Conv2d(planes,planes * self.expansion,1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        #print_shape(self.bn3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample= None \n",
    "        self.stride = stride \n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            self.downsample = nn.Sequential(OrderedDict([ \n",
    "                (\"-1\",nn.AvgPool2d(stride)),\n",
    "                (\"0\",nn.Conv2d(inplanes,planes * self.expansion, 1 , stride = 1, bias= False)),\n",
    "                (\"1\",nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "            #print_shape(self.downsample)\n",
    "            \n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        identity = x \n",
    "        print_shape(x)\n",
    "        #conv1 -> bn1 -> relu\n",
    "        out = self.relu(self.bn1(self.conv1(x)))   \n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity \n",
    "        out = self.relu(out)\n",
    "        print(\"out shape :\",end=' ')\n",
    "        print_shape(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current shape torch.Size([12, 256, 256, 1])\n",
      "out shape : current shape torch.Size([12, 256, 256, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 256, 256, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bottleneck = Bottleneck(inplanes = 256, planes = 64 , stride=1)\n",
    "inputs = torch.rand((12,256,256,1))\n",
    "test_bottleneck.forward(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP4ClipPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP4ClipPreTrainedModel(PreTrainedModel, nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, cross_config, *inputs, **kwargs):\n",
    "        super(CLIP4ClipPreTrainedModel, self).__init__(cross_config)\n",
    "        self.cross_config = cross_config\n",
    "        self.clip = None\n",
    "        self.cross = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cross_model_name, state_dict=None, cache_dir=None, type_vocab_size=2, *inputs, **kwargs):\n",
    "\n",
    "        task_config = None\n",
    "        if \"task_config\" in kwargs.keys():\n",
    "            task_config = kwargs[\"task_config\"]\n",
    "            ###### 20220524 local rank\n",
    "            if not hasattr(task_config, \"local_rank\"):\n",
    "                #args.__dict__[\"max_frames\"]출력하면 100 나옴을 확인\n",
    "                task_config.__dict__[\"local_rank\"] = 0\n",
    "            elif task_config.local_rank == -1:\n",
    "                task_config.local_rank = 0\n",
    "\n",
    "        if state_dict is None: state_dict = {}\n",
    "        pretrained_clip_name = \"ViT-B/32\"\n",
    "        if hasattr(task_config, 'pretrained_clip_name'):\n",
    "            pretrained_clip_name = task_config.pretrained_clip_name\n",
    "        clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "        for key, val in clip_state_dict.items():\n",
    "            new_key = \"clip.\" + key\n",
    "            if new_key not in state_dict:\n",
    "                state_dict[new_key] = val.clone()\n",
    "\n",
    "        cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "\n",
    "        model = cls(cross_config, clip_state_dict, *inputs, **kwargs)\n",
    "\n",
    "        ## ===> Initialization trick [HARD CODE]\n",
    "        if model.linear_patch == \"3d\":\n",
    "            contain_conv2 = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"visual.conv2.weight\") > -1:\n",
    "                    contain_conv2 = True\n",
    "                    break\n",
    "            if contain_conv2 is False and hasattr(model.clip.visual, \"conv2\"):\n",
    "                cp_weight = state_dict[\"clip.visual.conv1.weight\"].clone()\n",
    "                kernel_size = model.clip.visual.conv2.weight.size(2)\n",
    "                conv2_size = model.clip.visual.conv2.weight.size()\n",
    "                conv2_size = list(conv2_size)\n",
    "\n",
    "                left_conv2_size = conv2_size.copy()\n",
    "                right_conv2_size = conv2_size.copy()\n",
    "                left_conv2_size[2] = (kernel_size - 1) // 2\n",
    "                right_conv2_size[2] = kernel_size - 1 - left_conv2_size[2]\n",
    "\n",
    "                left_zeros, right_zeros = None, None\n",
    "                if left_conv2_size[2] > 0:\n",
    "                    left_zeros = torch.zeros(*tuple(left_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "                if right_conv2_size[2] > 0:\n",
    "                    right_zeros = torch.zeros(*tuple(right_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "\n",
    "                cat_list = []\n",
    "                if left_zeros != None: cat_list.append(left_zeros)\n",
    "                cat_list.append(cp_weight.unsqueeze(2))\n",
    "                if right_zeros != None: cat_list.append(right_zeros)\n",
    "                cp_weight = torch.cat(cat_list, dim=2)\n",
    "\n",
    "                state_dict[\"clip.visual.conv2.weight\"] = cp_weight\n",
    "\n",
    "        if model.sim_header == 'tightTransf':\n",
    "            contain_cross = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"cross.transformer\") > -1:\n",
    "                    contain_cross = True\n",
    "                    break\n",
    "            if contain_cross is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"cross.embeddings.position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[\"cross.\"+key] = val.clone()\n",
    "                            continue\n",
    "\n",
    "        if model.sim_header == \"seqLSTM\" or model.sim_header == \"seqTransf\":\n",
    "            contain_frame_position = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"frame_position_embeddings\") > -1:\n",
    "                    contain_frame_position = True\n",
    "                    break\n",
    "            if contain_frame_position is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"frame_position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if model.sim_header == \"seqTransf\" and key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[key.replace(\"transformer.\", \"transformerClip.\")] = val.clone()\n",
    "                            continue\n",
    "        ## <=== End of initialization trick\n",
    "\n",
    "        if state_dict is not None:\n",
    "            model = cls.init_preweight(model, state_dict, task_config=task_config)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_config와 Target_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_log(task_config, info):\n",
    "    if task_config is None or task_config.local_rank == 0:\n",
    "        logger.warning(info)\n",
    "\n",
    "def update_attr(target_name, target_config, target_attr_name, source_config, source_attr_name, default_value=None):\n",
    "    if hasattr(source_config, source_attr_name):\n",
    "        if default_value is None or getattr(source_config, source_attr_name) != default_value:\n",
    "            setattr(target_config, target_attr_name, getattr(source_config, source_attr_name))\n",
    "            show_log(source_config, \"Set {}.{}: {}.\".format(target_name,\n",
    "                                                            target_attr_name, getattr(target_config, target_attr_name)))\n",
    "    return target_config\n",
    "\n",
    "def check_attr(target_name, task_config):\n",
    "    return hasattr(task_config, target_name) and task_config.__dict__[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALoader 생성하여 video의 shape(6차원)을 확인 \n",
    "video shape : \n",
    " - Pair : 1\n",
    " - max_frame : 100 \n",
    " - batch : 1 \n",
    " - Channel : 3 \n",
    " - H : 224\n",
    " - W : 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_msvd_train(args,tokenizer):\n",
    "    msvd_dataset=MSVD_DataLoader(\n",
    "        subset = \"train\",\n",
    "        data_path = args.data_path,\n",
    "        features_path = args.features_path,\n",
    "        max_words = args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer = tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order = args.train_frame_order, \n",
    "        slice_framepos = args.slice_framepos\n",
    "    )\n",
    "\n",
    "    #train_sampler = torch.utils.data.distributed.DistributedSampler(msvd_dataset)\n",
    "    train_sampler = 0\n",
    "    dataloader = DataLoader(\n",
    "        msvd_dataset,\n",
    "        batch_size = args.batch_size // args.n_gpu, \n",
    "        num_workers = args.num_thread_reader,\n",
    "        pin_memory=False, \n",
    "        shuffle = (train_sampler is None), \n",
    "        sampler = train_sampler, \n",
    "        drop_last=True,\n",
    "\n",
    "        \n",
    "    )\n",
    "\n",
    "    return msvd_dataset, dataloader, len(msvd_dataset),train_sampler\n",
    "\n",
    "def dataloader_msvd_test(args, tokenizer, subset=\"test\"):\n",
    "    msvd_testset = MSVD_DataLoader(\n",
    "        subset=subset,\n",
    "        data_path=args.data_path,\n",
    "        features_path=args.features_path,\n",
    "        max_words=args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer=tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order=args.eval_frame_order,\n",
    "        slice_framepos=args.slice_framepos,\n",
    "    )\n",
    "    dataloader_msvd = DataLoader(\n",
    "        msvd_testset,\n",
    "        batch_size=args.batch_size_val,\n",
    "        num_workers=args.num_thread_reader,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return msvd_testset, dataloader_msvd, len(msvd_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video number: 1200\n",
      "Total Paire: 48774\n",
      "For test, sentence number: 27763\n",
      "For test, video number: 670\n",
      "Video number: 670\n",
      "Total Paire: 27763\n"
     ]
    }
   ],
   "source": [
    "msvd_dataset,train_dataloader,len_of_msvdtrain,train_sampler = dataloader_msvd_train(args,tokenizer)\n",
    "msvd_testset, test_dataloader, len_of_msvdtest = dataloader_msvd_test(args,tokenizer,subset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_PATH와  FEATURE_PATH를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PATH : /home/key2317/CLIP4Clip/msvd_data\n",
      "FEATURE PATH  /home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA PATH :\",DATA_PATH)\n",
    "print(\"FEATURE PATH \",FEATURES_PATH)\n",
    "video_id_path_dict = {}\n",
    "video_id_path_dict[\"train\"] = os.path.join(DATA_PATH, \"train_list.txt\")\n",
    "video_id_path_dict[\"val\"] = os.path.join(DATA_PATH, \"val_list.txt\")\n",
    "video_id_path_dict[\"test\"] = os.path.join(DATA_PATH, \"test_list.txt\")\n",
    "caption_file = os.path.join(DATA_PATH, \"raw-captions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "captions : video_id와 대사로 이루어진 dict\n",
    "\n",
    "video_ids : video_id를 모아놓은 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caption_file,'rb') as f:\n",
    "    captions = pickle.load(f)\n",
    "\n",
    "with open(video_id_path_dict[\"train\"], 'r') as fp:\n",
    "    video_ids = [itm.strip() for itm in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(captions)\n",
    "# print(video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video_dict : 파일과 video_id로 이루어진 dict \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {} \n",
    "for root, dub_dir,video_files in os.walk(args.features_path):\n",
    "    for video_file in video_files:\n",
    "        video_id_ = \".\".join(video_file.split(\".\")[:-1])\n",
    "        if video_id_ not in video_ids:\n",
    "            continue\n",
    "        file_path_ = os.path.join(root,video_file)\n",
    "        video_dict[video_id_] = file_path_ \n",
    "\n",
    "#print(video_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences_dict : key : 인덱스 / value : (video_id, 문장) 으로 이루어진 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48774\n"
     ]
    }
   ],
   "source": [
    "sentences_dict = {} \n",
    "cut_off_points=[] \n",
    "for video_id in video_ids:\n",
    "    assert video_id in captions \n",
    "    for cap in captions[video_id]:\n",
    "        cap_txt = \" \".join(cap)\n",
    "        sentences_dict[len(sentences_dict)] = (video_id,cap_txt)\n",
    "    cut_off_points.append(len(sentences_dict))\n",
    "#print(sentences_dict)\n",
    "#print(cut_off_points)\n",
    "print(len(sentences_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msvd_dataset에서 getitem을 할 때, \n",
    " - get_text의 shape\n",
    " - get_video의 shape\n",
    "\n",
    "를 확인하는 작업입니다.\n",
    "\n",
    "dataset에서 _getitem_을 통해 인덱스 하나에 대하여 video와 text를 각각 꺼내옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4wsuPCjDBc_5_15\n",
      "a chipmunk is eating\n",
      "current shape (1, 30)\n",
      "current shape (1, 30)\n",
      "current shape (1, 30)\n",
      "['-4wsuPCjDBc_5_15']\n"
     ]
    }
   ],
   "source": [
    "idx = 1 \n",
    "video_id,caption = sentences_dict[idx]\n",
    "print(video_id)\n",
    "print(caption)\n",
    "pairs_text, pairs_mask, pairs_segment, choice_video_ids  = msvd_dataset._get_text(video_id,caption)\n",
    "print_shape(pairs_text)\n",
    "print_shape(pairs_mask)\n",
    "print_shape(pairs_segment)\n",
    "print(choice_video_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_video_data_clip shape : torch.Size([11, 3, 224, 224])\n",
      "raw_video_slice shape : torch.Size([11, 1, 3, 224, 224])\n",
      "(1, 100, 1, 3, 224, 224)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "video, video_mask = msvd_dataset._get_rawvideo(choice_video_ids)\n",
    "print(video.shape)\n",
    "print(video_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshape(pairs_text,pairs_mask,pairs_segment,video,video_mask):\n",
    "    print_shape(pairs_text)\n",
    "    print_shape(pairs_mask)\n",
    "    print_shape(pairs_segment)\n",
    "    print_shape(video)\n",
    "    print_shape(video_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_video_data_clip shape : torch.Size([11, 3, 224, 224])\n",
      "raw_video_slice shape : torch.Size([11, 1, 3, 224, 224])\n",
      "current shape (1, 30)\n",
      "current shape (1, 30)\n",
      "current shape (1, 30)\n",
      "current shape (1, 100, 1, 3, 224, 224)\n",
      "current shape (1, 100)\n"
     ]
    }
   ],
   "source": [
    "pairs_text,pairs_mask,pairs_segment,video,video_mask = msvd_dataset.__getitem__(1)\n",
    "getshape(pairs_text,pairs_mask,pairs_segment,video,video_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacial_dim = 224 \n",
    "embed_dim = 3 \n",
    "num_heads = 3\n",
    "output_dim = 512 \n",
    "#224 * 224 = 50176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self, spacial_dim: int, embed_dim : int, num_heads : int, output_dim : int = None):\n",
    "        super(AttentionPool2d,self).__init__()\n",
    "        #spacial_dim의 제곱 + 1 , embed_dim \n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 +1 ,embed_dim) / embed_dim ** 0.5)\n",
    "        print(self.positional_embedding.shape)\n",
    "        # Key, Query, Value\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(embed_dim,output_dim or embed_dim)\n",
    "        self.num_heads = num_heads \n",
    "\n",
    "    def forward(self,x):\n",
    "        ###################### 차원 한번 줄임 ######################\n",
    "        x = x.reshape(x.shape[0],x.shape[1],x.shape[2] * x.shape[3]).permute(2,0,1) # NCHW -> (HW)NC\n",
    "        print('permuted x shape : {}'.format(x.shape))\n",
    "        x = torch.cat([x.mean(dim=0,keepdim=True),x],dim=0) #(HW+1)NC\n",
    "        print('torch cat x shape :',x.shape) #(224 * 224 +1 , 1, 3)\n",
    "        x = x + self.positional_embedding[:,None, :].to(x.dtype) # (HW+1)NC\n",
    "        print(\"added with positional_embedding shape :\",x.shape)\n",
    "        ## multi head attention 수행\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query = x, key = x, value = x, \n",
    "            \n",
    "            embed_dim_to_check = x.shape[-1],\n",
    "            num_heads = self.num_heads, \n",
    "\n",
    "            q_proj_weight = self.q_proj.weight, #nn.Linear(embed_dim,embed_dim) 의 weight\n",
    "            k_proj_weight = self.k_proj.weight, #Linear \n",
    "            v_proj_weight = self.v_proj.weight, \n",
    "            in_proj_weight = None, \n",
    "            in_proj_bias = torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), \n",
    "            bias_k = None,\n",
    "            bias_v = None, \n",
    "            add_zero_attn = False, \n",
    "            dropout_p= 0, \n",
    "            out_proj_weight=self.c_proj.weight, \n",
    "            out_proj_bias= self.c_proj.bias, \n",
    "            use_separate_proj_weight=True, \n",
    "            training = self.training, \n",
    "            need_weights = False\n",
    "        )\n",
    "\n",
    "        return x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50177, 3])\n",
      "permuted x shape : torch.Size([50176, 1, 3])\n",
      "torch cat x shape : torch.Size([50177, 1, 3])\n",
      "added with positional_embedding shape : torch.Size([50177, 1, 3])\n",
      "returned : torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "attnp2d = AttentionPool2d(spacial_dim,embed_dim,num_heads,output_dim)\n",
    "x = torch.rand(1,3,224,224)\n",
    "\n",
    "res = attnp2d.forward(x)\n",
    "print(\"returned :\",res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttentionPool2d의 spec \n",
    " - spacial_dim : 224 \n",
    " - embed_dim : 3 \n",
    " - num_heads : 3 \n",
    " - output_dim : 512 \n",
    " - embedding 의 shape : (spacial_dim) * (spacial_dim) , 1 , 3 \n",
    "\n",
    " - input x의 shape : (N : 1, C : 3, H : 224, W : 224)\n",
    " - output x[0]의 shape : 1 , output_dim 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "\n",
    "    def forward(self,x : torch.Tensor):\n",
    "        orig_type = x.dtype \n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReisudalAttentionBlock : 크게 다를 것은 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    # d_model이 Transformer에서 넘어온 width와 같음. \n",
    "    def __init__(self, d_model : int, n_head : int, attn_mask = None):\n",
    "        super(ResidualAttentionBlock,self).__init__()\n",
    "        \n",
    "        ##### attention Block\n",
    "        self.attn = nn.MultiheadAttention(d_model,n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\",nn.Linear(d_model,d_model*4)),\n",
    "            (\"gelu\",QuickGELU()),\n",
    "            (\"c_proj\",nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask \n",
    "    \n",
    "    def attention(self,x:torch.Tensor):\n",
    "        attn_mask_ = self.attn_mask \n",
    "        if self.attn_mask is not None and hasattr(self.attn_mask, '__call__'):\n",
    "            attn_mask_ = self.attn_mask(x.size(0))\n",
    "        attn_mask_ = attn_mask_.to(dtype=x.dtype,device = x.device) if attn_mask_ is not None else None \n",
    "        return self.attn(x,x,x,need_weights=False, attn_mask = attn_mask_)[0]\n",
    "    \n",
    "    def forward(self,x_tuple : tuple):\n",
    "        x,video_frame = x_tuple \n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return (x,video_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width :int, layers :int , heads :int, attn_mask = None):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.width = width \n",
    "        self.layers = layers \n",
    "        # layer의 숫자만큼 Residualattention을 진행\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width,heads,attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, video_frame = -1):\n",
    "        return self.resblocks((x,video_frame))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Autoencoder3D(nn.Module):\n",
    "#     def __init__(self, frames, ch, width, height):\n",
    "#         super(Autoencoder3D, self).__init__()\n",
    "#         self.frames = frames\n",
    "#         self.ch = ch\n",
    "#         self.width = width\n",
    "#         self.height = height\n",
    "#         def EncoderLayer(in_channel, out_channel, pooling_size=None):\n",
    "#             network = []\n",
    "#             network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "#             network += [nn.BatchNorm3d(out_channel)]\n",
    "#             network += [nn.LeakyReLU()]\n",
    "#             if pooling_size is not None:\n",
    "#                 network += [nn.MaxPool3d(kernel_size=pooling_size)]\n",
    "#             network = nn.Sequential(*network)\n",
    "#             return network\n",
    "#         def DecoderLayer(in_channel, out_channel, size=None):\n",
    "#             network = []\n",
    "#             if size is None:\n",
    "#                 network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "#             else:\n",
    "#                 network += [nn.ConvTranspose3d(in_channel, out_channel, kernel_size=size, padding=0, stride=size)]\n",
    "#             network += [nn.BatchNorm3d(out_channel)]\n",
    "#             network += [nn.LeakyReLU()]\n",
    "#             network = nn.Sequential(*network)\n",
    "#             return network\n",
    "#         self.encoder1 = EncoderLayer(3, 32, pooling_size=2)\n",
    "#         self.encoder2 = EncoderLayer(32, 48, pooling_size=2)\n",
    "#         self.encoder3 = EncoderLayer(48, 64)\n",
    "#         self.decoder1 = DecoderLayer(64, 48)\n",
    "#         self.decoder2 = DecoderLayer(48, 32, size=2)\n",
    "#         self.decoder3 = DecoderLayer(32, 3, size=2)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#     def forward(self, x):\n",
    "#         encoded = self.encoder3(self.encoder2(self.encoder1(x)))\n",
    "#         print('current encoded shape :',encoded.shape)\n",
    "#         decoded = self.decoder3(self.decoder2(self.decoder1(encoded)))\n",
    "#         decoded = self.sigmoid(decoded)\n",
    "#         return decoded\n",
    "#     def compute_loss(self, x, y):\n",
    "#         mse = nn.MSELoss()\n",
    "#         loss = mse(x, y)\n",
    "#         return loss, loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisionTransformer : \n",
    "\n",
    "transformer_width : clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "\n",
    "transformer_heads : transformer_width// 64 64등분!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self,input_resolution : int, patch_size : int, width : int, layers : int, heads : int, output_dim : int, linear_patch : str = '2d',):\n",
    "        super(VisualTransformer,self).__init__()\n",
    "        self.input_resolution = input_resolution \n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        ##### 2D일 때에는 Conv1d\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels = width, kernel_size = patch_size,stride=patch_size,bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size)**2 +1, width)) \n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "\n",
    "        #Transformer 인자 1 : width / 인자 2 : layers / 인자 3 : heads \n",
    "        self.transformer = Transformer(width,layers,heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width,output_dim))   \n",
    "\n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch \n",
    "        if self.linear_patch == '3d':\n",
    "            #### 3D일 때에는 Conv2d\n",
    "            self.conv2 = nn.Conv3d(in_channels=3, out_channels=width, kernel_size = (3,patch_size,patch_size), \n",
    "            stride = (1,patch_size,patch_size),padding = (1,0,0),bias= False)\n",
    "\n",
    "    def forward(self, x : torch.Tensor,video_frame=-1):\n",
    "\n",
    "        if self.linear_patch == '3d':\n",
    "            assert video_frame !=-1\n",
    "            print(\" input x's shape :\",x.shape)\n",
    "\n",
    "            x_3d = x.reshape(-1,video_frame,x.shape[-3],x.shape[-2],x.shape[-1])\n",
    "            print('reshaped x shape :',x_3d.shape)\n",
    "            x_3d = x_3d.permute(0,2,1,3,4) \n",
    "            print('permuted(0,2,1,3,4) shape :',x_3d.shape) \n",
    "            x = x_3d.reshape(-1,x_3d.shape[-3],x_3d.shape[-2],x_3d.shape[-1]).contiguous()\n",
    "\n",
    "        else:\n",
    "            # 2d일 때\n",
    "            x = self.conv1(x)\n",
    "        \n",
    "        ######################### 여기까지 3D이냐 2D이냐에 따라서 x를 구하는 작업임 #########################\n",
    "\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1) \n",
    "        print(\" start x's shape :\",x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        print(\"permuted(0,2,1) shape :\",x.shape)\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        print(\"torch cat x shape :\",x.shape)\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        print(\"positional embedding x shape :\",x.shape)\n",
    "        x = self.ln_pre(x)\n",
    "        print(\"layernormed x shape :\",x.shape)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        print(\"permuted(1,0,2) x shape :\",x.shape)\n",
    "        x = self.transformer(x, video_frame=video_frame)\n",
    "        print(\"transformer x shape :\",x.shape)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        print(\"permuted(1,0,2) x shape :\",x.shape)\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP 해부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CLIP 초기화에 필요한 Vision parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vision_layers = len([k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "image_resolution = vision_patch_size * grid_size\n",
    "vision_heads = vision_width //64\n",
    "linear_patch = '2d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CLIP 초기화에 필요한 text parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "transformer_heads = transformer_width // 64\n",
    "#transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\"))) #12\n",
    "transformer_layers = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nn.Embedding(vocab_size,transformer_width)\n",
    "positional_embedding = nn.Parameter(torch.empty(context_length,transformer_width))\n",
    "ln_final = LayerNorm(transformer_width)\n",
    "text_projection = nn.Parameter(torch.empty(transformer_width,embed_dim))\n",
    "logit_scale = nn.Parameter(torch.ones([]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CLIP 내부에 있는 모든 ViT, text transformer 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### visual transformer\n",
    "visual = VisualTransformer(\n",
    "    input_resolution = image_resolution, \n",
    "    patch_size=vision_patch_size,\n",
    "    width=vision_width,\n",
    "    layers=vision_layers,\n",
    "    heads=vision_heads,\n",
    "    output_dim=embed_dim,\n",
    "    linear_patch=linear_patch    \n",
    ")\n",
    "\n",
    "def build_attention_mask(context_length):\n",
    "# lazily create causal attention mask, with full attention between the vision tokens\n",
    "# pytorch uses additive attention mask; fill with -inf\n",
    "    mask = torch.zeros(context_length, context_length)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    return mask\n",
    "\n",
    "\n",
    "############ Text transformer\n",
    "transformer = Transformer(\n",
    "    width=transformer_width,\n",
    "    layers=transformer_layers,\n",
    "    heads=transformer_heads,\n",
    "    attn_mask=build_attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text와 image를 Encode하는 함수, 그리고 이를 받아서 forward하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def dtype():\n",
    "    return visual.conv1.weight.dtype\n",
    "\n",
    "def encode_image(image, return_hidden=False, video_frame=-1):\n",
    "    hidden = visual(image.type(dtype), video_frame=video_frame)\n",
    "    hidden = visual.ln_post(hidden) @ visual.proj\n",
    "\n",
    "    x = hidden[:, 0, :]\n",
    "\n",
    "    if return_hidden:\n",
    "        return x, hidden\n",
    "\n",
    "    return x\n",
    "\n",
    "def encode_text(text, return_hidden=False):\n",
    "    x = token_embedding(text).type(dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "    pos_emd = positional_embedding[:x.size(1), :].type(dtype)\n",
    "    x = x + pos_emd\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    x = transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "    hidden = ln_final(x).type(dtype) @ text_projection\n",
    "\n",
    "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "    x = hidden[torch.arange(hidden.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "    if return_hidden:\n",
    "        return x, hidden\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward(image, text):\n",
    "    image_features = encode_image(image)\n",
    "    text_features = encode_text(text)\n",
    "\n",
    "    # normalized features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder3D(nn.Module):\n",
    "    def __init__(self, frames, ch, width, height):\n",
    "        super(Autoencoder3D, self).__init__()\n",
    "        self.frames = frames\n",
    "        self.ch = ch\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        def EncoderLayer(in_channel, out_channel, pooling_size=None):\n",
    "            network = []\n",
    "            network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            if pooling_size is not None:\n",
    "                network += [nn.MaxPool3d(kernel_size=pooling_size)]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        def DecoderLayer(in_channel, out_channel, size=None):\n",
    "            network = []\n",
    "            if size is None:\n",
    "                network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            else:\n",
    "                network += [nn.ConvTranspose3d(in_channel, out_channel, kernel_size=size, padding=0, stride=size)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        self.encoder1 = EncoderLayer(3, 32, pooling_size=2)\n",
    "        self.encoder2 = EncoderLayer(32, 48, pooling_size=2)\n",
    "        self.encoder3 = EncoderLayer(48, 64)\n",
    "        self.decoder1 = DecoderLayer(64, 48)\n",
    "        self.decoder2 = DecoderLayer(48, 32, size=2)\n",
    "        self.decoder3 = DecoderLayer(32, 3, size=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder3(self.encoder2(self.encoder1(x)))\n",
    "        print('current encoded shape :',encoded.shape)\n",
    "        decoded = self.decoder3(self.decoder2(self.decoder1(encoded)))\n",
    "        decoded = self.sigmoid(decoded)\n",
    "        return decoded\n",
    "    def compute_loss(self, x, y):\n",
    "        mse = nn.MSELoss()\n",
    "        loss = mse(x, y)\n",
    "        return loss, loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.rand(10, 3, 224, 224, 12)\n",
    "# set = Autoencoder3D(12, 3, 224, 224)\n",
    "# result = set(data)\n",
    "# print(result.shape)\n",
    "# loss, loss_item =set.compute_loss(data, result)\n",
    "# print(loss, loss_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisualTransformer 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisualTransformer(input_resolution = image_resolution, patch_size = vision_patch_size, width = vision_width, layers = vision_layers,heads= vision_heads,output_dim=embed_dim, linear_patch=linear_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Transformer 객체\n",
    " - forward 시 input x에 대하여 residual attention을 한다. \n",
    " - residual attention x_tuple에 대하여 \n",
    " - x,video_frame = x_tuple \n",
    " - x에 대해서는 ln -> attention -> mlp를 수행한다. \n",
    " - video_frame에 대해서는 아무것도 취하지 않고 이를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = Transformer(width = transformer_width, layers = transformer_layers, heads = transformer_heads, attn_mask= build_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eff65c0def8c12c365a8ff92cc515e5fbadd507d935c66604f35c789f0fc4099"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CLIP4Clip': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
