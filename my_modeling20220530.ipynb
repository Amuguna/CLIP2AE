{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modules.until_module import PreTrainedModel, AllGather, CrossEn\n",
    "from modules.module_cross import CrossModel, CrossConfig, Transformer as TransformerClip\n",
    "\n",
    "from modules.module_clip import CLIP, convert_weights\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import easydict\n",
    "global logger\n",
    "logger = logging.getLogger(__name__)\n",
    "allgather = AllGather.apply\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_TrainDataLoader\n",
    "from dataloaders.dataloader_msvd_retrieval import MSVD_DataLoader\n",
    "from dataloaders.dataloader_lsmdc_retrieval import LSMDC_DataLoader\n",
    "from dataloaders.dataloader_activitynet_retrieval import ActivityNet_DataLoader\n",
    "from dataloaders.dataloader_didemo_retrieval import DiDeMo_DataLoader\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from dataloaders.data_dataloaders import DATALOADER_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 중요 변수 설명 \n",
    "\n",
    "task_config : args\n",
    "\n",
    "clip_state_dict : ViT-B/32에 있는 parameter 목록 \n",
    "\n",
    "type_vocab_size : 2로 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ''\n",
    "cross_model_name = 'cross-base'\n",
    "n_gpu=1\n",
    "cache_dir=\"\"\n",
    "pretrained_clip_name = \"ViT-B/32\"\n",
    "clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "CONFIG_NAME = 'cross_config.json'\n",
    "#print(clip_state_dict.keys())\n",
    "action = 'store_true'\n",
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":16,\n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":torch.cuda.device_count(), #default :1\n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "    \"local_rank\":0,\n",
    "})\n",
    "type_vocab_size = 2 \n",
    "task_config = args\n",
    "cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "#print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP4ClipPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP4ClipPreTrainedModel(PreTrainedModel, nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, cross_config, *inputs, **kwargs):\n",
    "        super(CLIP4ClipPreTrainedModel, self).__init__(cross_config)\n",
    "        self.cross_config = cross_config\n",
    "        self.clip = None\n",
    "        self.cross = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cross_model_name, state_dict=None, cache_dir=None, type_vocab_size=2, *inputs, **kwargs):\n",
    "\n",
    "        task_config = None\n",
    "        if \"task_config\" in kwargs.keys():\n",
    "            task_config = kwargs[\"task_config\"]\n",
    "            if not hasattr(task_config, \"local_rank\"):\n",
    "                task_config.__dict__[\"local_rank\"] = 0\n",
    "            elif task_config.local_rank == -1:\n",
    "                task_config.local_rank = 0\n",
    "\n",
    "        if state_dict is None: state_dict = {}\n",
    "        pretrained_clip_name = \"ViT-B/32\"\n",
    "        if hasattr(task_config, 'pretrained_clip_name'):\n",
    "            pretrained_clip_name = task_config.pretrained_clip_name\n",
    "        clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "        for key, val in clip_state_dict.items():\n",
    "            new_key = \"clip.\" + key\n",
    "            if new_key not in state_dict:\n",
    "                state_dict[new_key] = val.clone()\n",
    "\n",
    "        cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "\n",
    "        model = cls(cross_config, clip_state_dict, *inputs, **kwargs)\n",
    "\n",
    "        ## ===> Initialization trick [HARD CODE]\n",
    "        if model.linear_patch == \"3d\":\n",
    "            contain_conv2 = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"visual.conv2.weight\") > -1:\n",
    "                    contain_conv2 = True\n",
    "                    break\n",
    "            if contain_conv2 is False and hasattr(model.clip.visual, \"conv2\"):\n",
    "                cp_weight = state_dict[\"clip.visual.conv1.weight\"].clone()\n",
    "                kernel_size = model.clip.visual.conv2.weight.size(2)\n",
    "                conv2_size = model.clip.visual.conv2.weight.size()\n",
    "                conv2_size = list(conv2_size)\n",
    "\n",
    "                left_conv2_size = conv2_size.copy()\n",
    "                right_conv2_size = conv2_size.copy()\n",
    "                left_conv2_size[2] = (kernel_size - 1) // 2\n",
    "                right_conv2_size[2] = kernel_size - 1 - left_conv2_size[2]\n",
    "\n",
    "                left_zeros, right_zeros = None, None\n",
    "                if left_conv2_size[2] > 0:\n",
    "                    left_zeros = torch.zeros(*tuple(left_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "                if right_conv2_size[2] > 0:\n",
    "                    right_zeros = torch.zeros(*tuple(right_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "\n",
    "                cat_list = []\n",
    "                if left_zeros != None: cat_list.append(left_zeros)\n",
    "                cat_list.append(cp_weight.unsqueeze(2))\n",
    "                if right_zeros != None: cat_list.append(right_zeros)\n",
    "                cp_weight = torch.cat(cat_list, dim=2)\n",
    "\n",
    "                state_dict[\"clip.visual.conv2.weight\"] = cp_weight\n",
    "\n",
    "        if model.sim_header == 'tightTransf':\n",
    "            contain_cross = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"cross.transformer\") > -1:\n",
    "                    contain_cross = True\n",
    "                    break\n",
    "            if contain_cross is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"cross.embeddings.position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[\"cross.\"+key] = val.clone()\n",
    "                            continue\n",
    "\n",
    "        if model.sim_header == \"seqLSTM\" or model.sim_header == \"seqTransf\":\n",
    "            contain_frame_position = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"frame_position_embeddings\") > -1:\n",
    "                    contain_frame_position = True\n",
    "                    break\n",
    "            if contain_frame_position is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"frame_position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if model.sim_header == \"seqTransf\" and key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[key.replace(\"transformer.\", \"transformerClip.\")] = val.clone()\n",
    "                            continue\n",
    "        ## <=== End of initialization trick\n",
    "\n",
    "        if state_dict is not None:\n",
    "            model = cls.init_preweight(model, state_dict, task_config=task_config)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_log(task_config, info):\n",
    "    if task_config is None or task_config.local_rank == 0:\n",
    "        logger.warning(info)\n",
    "\n",
    "def update_attr(target_name, target_config, target_attr_name, source_config, source_attr_name, default_value=None):\n",
    "    if hasattr(source_config, source_attr_name):\n",
    "        if default_value is None or getattr(source_config, source_attr_name) != default_value:\n",
    "            setattr(target_config, target_attr_name, getattr(source_config, source_attr_name))\n",
    "            show_log(source_config, \"Set {}.{}: {}.\".format(target_name,\n",
    "                                                            target_attr_name, getattr(target_config, target_attr_name)))\n",
    "    return target_config\n",
    "\n",
    "def check_attr(target_name, task_config):\n",
    "    return hasattr(task_config, target_name) and task_config.__dict__[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input : cross_config, clip_state_dict, task_config\n",
    "class CLIP4Clip(CLIP4ClipPreTrainedModel):\n",
    "    def __init__(self, cross_config, clip_state_dict, task_config):\n",
    "        super(CLIP4Clip, self).__init__(cross_config)\n",
    "        self.task_config = task_config\n",
    "        self.ignore_video_index = -1\n",
    "\n",
    "        assert self.task_config.max_words + self.task_config.max_frames <= cross_config.max_position_embeddings\n",
    "\n",
    "        self._stage_one = True\n",
    "        self._stage_two = False\n",
    "\n",
    "        show_log(task_config, \"Stage-One:{}, Stage-Two:{}\".format(self._stage_one, self._stage_two))\n",
    "\n",
    "        self.loose_type = False\n",
    "        if self._stage_one and check_attr('loose_type', self.task_config):\n",
    "            self.loose_type = True\n",
    "            show_log(task_config, \"Test retrieval by loose type.\")\n",
    "\n",
    "        # CLIP Encoders: From OpenAI: CLIP [https://github.com/openai/CLIP] ===>\n",
    "        vit = \"visual.proj\" in clip_state_dict\n",
    "        assert vit\n",
    "        if vit:\n",
    "            vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "            vision_layers = len(\n",
    "                [k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "            vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "            grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            image_resolution = vision_patch_size * grid_size\n",
    "        else:\n",
    "            counts: list = [len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"visual.layer{b}\"))) for b in\n",
    "                            [1, 2, 3, 4]]\n",
    "            vision_layers = tuple(counts)\n",
    "            vision_width = clip_state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "            output_width = round((clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            vision_patch_size = None\n",
    "            assert output_width ** 2 + 1 == clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "            image_resolution = output_width * 32\n",
    "\n",
    "        embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "        context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "        vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "        transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "        transformer_heads = transformer_width // 64\n",
    "        transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "        show_log(task_config, \"\\t embed_dim: {}\".format(embed_dim))\n",
    "        show_log(task_config, \"\\t image_resolution: {}\".format(image_resolution))\n",
    "        show_log(task_config, \"\\t vision_layers: {}\".format(vision_layers))\n",
    "        show_log(task_config, \"\\t vision_width: {}\".format(vision_width))\n",
    "        show_log(task_config, \"\\t vision_patch_size: {}\".format(vision_patch_size))\n",
    "        show_log(task_config, \"\\t context_length: {}\".format(context_length))\n",
    "        show_log(task_config, \"\\t vocab_size: {}\".format(vocab_size))\n",
    "        show_log(task_config, \"\\t transformer_width: {}\".format(transformer_width))\n",
    "        show_log(task_config, \"\\t transformer_heads: {}\".format(transformer_heads))\n",
    "        show_log(task_config, \"\\t transformer_layers: {}\".format(transformer_layers))\n",
    "\n",
    "        self.linear_patch = '2d'\n",
    "        if hasattr(task_config, \"linear_patch\"):\n",
    "            self.linear_patch = task_config.linear_patch\n",
    "            show_log(task_config, \"\\t\\t linear_patch: {}\".format(self.linear_patch))\n",
    "\n",
    "        # use .float() to avoid overflow/underflow from fp16 weight. https://github.com/openai/CLIP/issues/40\n",
    "        cut_top_layer = 0\n",
    "        show_log(task_config, \"\\t cut_top_layer: {}\".format(cut_top_layer))\n",
    "        self.clip = CLIP(\n",
    "            embed_dim,\n",
    "            image_resolution, vision_layers-cut_top_layer, vision_width, vision_patch_size,\n",
    "            context_length, vocab_size, transformer_width, transformer_heads, transformer_layers-cut_top_layer,\n",
    "            linear_patch=self.linear_patch\n",
    "        ).float()\n",
    "\n",
    "        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "            if key in clip_state_dict:\n",
    "                del clip_state_dict[key]\n",
    "\n",
    "        convert_weights(self.clip)\n",
    "        # <=== End of CLIP Encoders\n",
    "\n",
    "        self.sim_header = 'meanP'\n",
    "        if hasattr(task_config, \"sim_header\"):\n",
    "            self.sim_header = task_config.sim_header\n",
    "            show_log(task_config, \"\\t sim_header: {}\".format(self.sim_header))\n",
    "        if self.sim_header == \"tightTransf\": assert self.loose_type is False\n",
    "\n",
    "        cross_config.max_position_embeddings = context_length\n",
    "        if self.loose_type is False:\n",
    "            # Cross Encoder ===>\n",
    "            cross_config = update_attr(\"cross_config\", cross_config, \"num_hidden_layers\", self.task_config, \"cross_num_hidden_layers\")\n",
    "            self.cross = CrossModel(cross_config)\n",
    "            # <=== End of Cross Encoder\n",
    "            self.similarity_dense = nn.Linear(cross_config.hidden_size, 1)\n",
    "\n",
    "        if self.sim_header == \"seqLSTM\" or self.sim_header == \"seqTransf\":\n",
    "            self.frame_position_embeddings = nn.Embedding(cross_config.max_position_embeddings, cross_config.hidden_size)\n",
    "        if self.sim_header == \"seqTransf\":\n",
    "            self.transformerClip = TransformerClip(width=transformer_width, layers=self.task_config.cross_num_hidden_layers,\n",
    "                                                   heads=transformer_heads, )\n",
    "        if self.sim_header == \"seqLSTM\":\n",
    "            self.lstm_visual = nn.LSTM(input_size=cross_config.hidden_size, hidden_size=cross_config.hidden_size,\n",
    "                                       batch_first=True, bidirectional=False, num_layers=1)\n",
    "\n",
    "        self.loss_fct = CrossEn()\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, video, video_mask=None):\n",
    "        input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        # T x 3 x H x W\n",
    "        video = torch.as_tensor(video).float()\n",
    "        b, pair, bs, ts, channel, h, w = video.shape\n",
    "        video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "        video_frame = bs * ts\n",
    "\n",
    "        sequence_output, visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask,\n",
    "                                                                         video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        if self.training:\n",
    "            loss = 0.\n",
    "            sim_matrix, *_tmp = self.get_similarity_logits(sequence_output, visual_output, attention_mask, video_mask,\n",
    "                                                    shaped=True, loose_type=self.loose_type)\n",
    "            sim_loss1 = self.loss_fct(sim_matrix)\n",
    "            sim_loss2 = self.loss_fct(sim_matrix.T)\n",
    "            sim_loss = (sim_loss1 + sim_loss2) / 2\n",
    "            loss += sim_loss\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_sequence_output(self, input_ids, token_type_ids, attention_mask, shaped=False):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "\n",
    "        bs_pair = input_ids.size(0)\n",
    "        sequence_hidden = self.clip.encode_text(input_ids).float()\n",
    "        sequence_hidden = sequence_hidden.view(bs_pair, -1, sequence_hidden.size(-1))\n",
    "\n",
    "        return sequence_hidden\n",
    "\n",
    "    def get_visual_output(self, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "\n",
    "        bs_pair = video_mask.size(0)\n",
    "        visual_hidden = self.clip.encode_image(video, video_frame=video_frame).float()\n",
    "        visual_hidden = visual_hidden.view(bs_pair, -1, visual_hidden.size(-1))\n",
    "\n",
    "        return visual_hidden\n",
    "\n",
    "    def get_sequence_visual_output(self, input_ids, token_type_ids, attention_mask, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "\n",
    "        sequence_output = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True)\n",
    "        visual_output = self.get_visual_output(video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        return sequence_output, visual_output\n",
    "\n",
    "    def _get_cross_output(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "\n",
    "        concat_features = torch.cat((sequence_output, visual_output), dim=1)  # concatnate tokens and frames\n",
    "        concat_mask = torch.cat((attention_mask, video_mask), dim=1)\n",
    "        text_type_ = torch.zeros_like(attention_mask)\n",
    "        video_type_ = torch.ones_like(video_mask)\n",
    "        concat_type = torch.cat((text_type_, video_type_), dim=1)\n",
    "\n",
    "        cross_layers, pooled_output = self.cross(concat_features, concat_type, concat_mask, output_all_encoded_layers=True)\n",
    "        cross_output = cross_layers[-1]\n",
    "\n",
    "        return cross_output, pooled_output, concat_mask\n",
    "\n",
    "    def _mean_pooling_for_similarity_sequence(self, sequence_output, attention_mask):\n",
    "        attention_mask_un = attention_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        attention_mask_un[:, 0, :] = 0.\n",
    "        sequence_output = sequence_output * attention_mask_un\n",
    "        text_out = torch.sum(sequence_output, dim=1) / torch.sum(attention_mask_un, dim=1, dtype=torch.float)\n",
    "        return text_out\n",
    "\n",
    "    def _mean_pooling_for_similarity_visual(self, visual_output, video_mask,):\n",
    "        video_mask_un = video_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        visual_output = visual_output * video_mask_un\n",
    "        video_mask_un_sum = torch.sum(video_mask_un, dim=1, dtype=torch.float)\n",
    "        video_mask_un_sum[video_mask_un_sum == 0.] = 1.\n",
    "        video_out = torch.sum(visual_output, dim=1) / video_mask_un_sum\n",
    "        return video_out\n",
    "    # mean_pooling_for_similarity = 위에 두 함수를 Nested하고 있습니다. \n",
    "    def _mean_pooling_for_similarity(self, sequence_output, visual_output, attention_mask, video_mask,):\n",
    "        text_out = self._mean_pooling_for_similarity_sequence(sequence_output, attention_mask)\n",
    "        video_out = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "\n",
    "        return text_out, video_out\n",
    "\n",
    "\n",
    "    #논문에서 Loose_ type에 들어가는 것은 Parameter free type인 meanP 이후 cosine similarity를 구하는 과정\n",
    "    #sequence type : sequence LSTM, 혹은 sequence Transformer을 거친 zi를 meanP한 zi와 wj 사이에서  cosine similarity가 있다고 설명함. \n",
    "    #이 부분은 3가지 similarity를 설명하는 부분에 잘 나오게 됩니다.\n",
    "    def _loose_similarity(self, sequence_output, visual_output, attention_mask, video_mask, sim_header=\"meanP\"):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        if sim_header == \"meanP\":\n",
    "            # Default: Parameter-free type\n",
    "            pass\n",
    "        elif sim_header == \"seqLSTM\":\n",
    "            # Sequential type: LSTM\n",
    "            visual_output_original = visual_output\n",
    "            visual_output = pack_padded_sequence(visual_output, torch.sum(video_mask, dim=-1).cpu(),\n",
    "                                                 batch_first=True, enforce_sorted=False)\n",
    "            visual_output, _ = self.lstm_visual(visual_output)\n",
    "            if self.training: self.lstm_visual.flatten_parameters()\n",
    "            visual_output, _ = pad_packed_sequence(visual_output, batch_first=True)\n",
    "            visual_output = torch.cat((visual_output, visual_output_original[:, visual_output.size(1):, ...].contiguous()), dim=1)\n",
    "            visual_output = visual_output + visual_output_original\n",
    "        elif sim_header == \"seqTransf\":\n",
    "            # Sequential type: Transformer Encoder\n",
    "            visual_output_original = visual_output\n",
    "            seq_length = visual_output.size(1)\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=visual_output.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(visual_output.size(0), -1)\n",
    "            frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
    "            visual_output = visual_output + frame_position_embeddings\n",
    "\n",
    "            extended_video_mask = (1.0 - video_mask.unsqueeze(1)) * -1000000.0\n",
    "            extended_video_mask = extended_video_mask.expand(-1, video_mask.size(1), -1)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # NLD -> LND\n",
    "            visual_output = self.transformerClip(visual_output, extended_video_mask)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # LND -> NLD\n",
    "            visual_output = visual_output + visual_output_original\n",
    "\n",
    "        if self.training:\n",
    "            visual_output = allgather(visual_output, self.task_config)\n",
    "            video_mask = allgather(video_mask, self.task_config)\n",
    "            sequence_output = allgather(sequence_output, self.task_config)\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "        visual_output = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sequence_output = sequence_output.squeeze(1)\n",
    "        sequence_output = sequence_output / sequence_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        retrieve_logits = logit_scale * torch.matmul(sequence_output, visual_output.t())\n",
    "        return retrieve_logits\n",
    "\n",
    "    def _cross_similarity(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        b_text, s_text, h_text = sequence_output.size()\n",
    "        b_visual, s_visual, h_visual = visual_output.size()\n",
    "\n",
    "        retrieve_logits_list = []\n",
    "\n",
    "        step_size = b_text      # set smaller to reduce memory cost\n",
    "        split_size = [step_size] * (b_text // step_size)\n",
    "        release_size = b_text - sum(split_size)\n",
    "        if release_size > 0:\n",
    "            split_size += [release_size]\n",
    "\n",
    "        # due to clip text branch retrun the last hidden\n",
    "        attention_mask = torch.ones(sequence_output.size(0), 1)\\\n",
    "            .to(device=attention_mask.device, dtype=attention_mask.dtype)\n",
    "\n",
    "        sequence_output_splits = torch.split(sequence_output, split_size, dim=0)\n",
    "        attention_mask_splits = torch.split(attention_mask, split_size, dim=0)\n",
    "        for i in range(len(split_size)):\n",
    "            sequence_output_row = sequence_output_splits[i]\n",
    "            attention_mask_row = attention_mask_splits[i]\n",
    "            sequence_output_l = sequence_output_row.unsqueeze(1).repeat(1, b_visual, 1, 1)\n",
    "            sequence_output_l = sequence_output_l.view(-1, s_text, h_text)\n",
    "            attention_mask_l = attention_mask_row.unsqueeze(1).repeat(1, b_visual, 1)\n",
    "            attention_mask_l = attention_mask_l.view(-1, s_text)\n",
    "\n",
    "            step_truth = sequence_output_row.size(0)\n",
    "            visual_output_r = visual_output.unsqueeze(0).repeat(step_truth, 1, 1, 1)\n",
    "            visual_output_r = visual_output_r.view(-1, s_visual, h_visual)\n",
    "            video_mask_r = video_mask.unsqueeze(0).repeat(step_truth, 1, 1)\n",
    "            video_mask_r = video_mask_r.view(-1, s_visual)\n",
    "\n",
    "            cross_output, pooled_output, concat_mask = \\\n",
    "                self._get_cross_output(sequence_output_l, visual_output_r, attention_mask_l, video_mask_r)\n",
    "            retrieve_logits_row = self.similarity_dense(pooled_output).squeeze(-1).view(step_truth, b_visual)\n",
    "\n",
    "            retrieve_logits_list.append(retrieve_logits_row)\n",
    "\n",
    "        retrieve_logits = torch.cat(retrieve_logits_list, dim=0)\n",
    "        return retrieve_logits\n",
    "\n",
    "    def get_similarity_logits(self, sequence_output, visual_output, attention_mask, video_mask, shaped=False, loose_type=False):\n",
    "        if shaped is False:\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        contrastive_direction = ()\n",
    "        if loose_type:\n",
    "            assert self.sim_header in [\"meanP\", \"seqLSTM\", \"seqTransf\"]\n",
    "            retrieve_logits = self._loose_similarity(sequence_output, visual_output, attention_mask, video_mask, sim_header=self.sim_header)\n",
    "        else:\n",
    "            assert self.sim_header in [\"tightTransf\"]\n",
    "            retrieve_logits = self._cross_similarity(sequence_output, visual_output, attention_mask, video_mask, )\n",
    "\n",
    "        return retrieve_logits, contrastive_direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP 해부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "transformer_heads = transformer_width // 64\n",
    "transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "vision_layers = len([k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "image_resolution = vision_patch_size * grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_layers = len([k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "image_resolution = vision_patch_size * grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "transformer_heads = transformer_width // 64\n",
    "transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nn.Embedding(vocab_size,transformer_width)\n",
    "positional_embedding = nn.Parameter(torch.empty(context_length,transformer_width))\n",
    "text_projection = nn.Parameter(torch.empty(transformer_width,embed_dim))\n",
    "logit_scale = nn.Parameter(torch.ones([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "16\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "print(task_config.max_words)\n",
    "print(task_config.max_frames)\n",
    "print(cross_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-One:True, Stage-Two:False\n",
      "\t embed_dim: 512\n",
      "\t image_resolution: 224\n",
      "\t vision_layers: 12\n",
      "\t vision_width: 768\n",
      "\t vision_patch_size: 32\n",
      "\t context_length: 77\n",
      "\t vocab_size: 49408\n",
      "\t transformer_width: 512\n",
      "\t transformer_heads: 8\n",
      "\t transformer_layers: 12\n",
      "\t cut_top_layer: 0\n"
     ]
    }
   ],
   "source": [
    "C4CPT = CLIP4ClipPreTrainedModel(cross_config)\n",
    "\n",
    "#clip_state_dict.keys()\n",
    "\n",
    "CLC = CLIP4Clip(cross_config = cross_config,clip_state_dict = clip_state_dict,task_config=task_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if ' _mean_pooling_for_similarity_visual' in dir(CLC):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
